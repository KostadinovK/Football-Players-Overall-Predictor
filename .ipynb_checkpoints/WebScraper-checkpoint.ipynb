{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraper for FIFA 19 data\n",
    "A WebScraper that scraps data from https://sofifa.com, a site with information and statistic about the footballers in one of the most successful sport simulation games of all times - FIFA. This project is focused on FIFA 19 so the scraper will only scrap data for FIFA 19.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web pages with the players are accessible via \"offset\" as a parameter in the link. So lets define our base url get info about the players __ID, Name, Age, Photo, Nationality, Flag, Overall, Potential, Club, Club Logo, Value, Wage and Special__ and put that info into a DataFrame called \"basic_player_data.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://sofifa.com/players?offset=\"\n",
    "offset = 0\n",
    "columns = ['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall', 'Potential', 'Club', \n",
    "           'Club Logo', 'Value', 'Wage', 'Special']\n",
    "\n",
    "data = pd.DataFrame(columns=columns)\n",
    "\n",
    "for offset in range(500): #we will get only the first 500 pages of footballers and it will need a lot of time :(\n",
    "    url = base_url + str(offset)\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text,\"html.parser\")\n",
    "    table_body = soup.find('tbody')\n",
    "    for row in table_body.findAll('tr'):\n",
    "        td = row.findAll('td')\n",
    "        picture = td[0].find('img').get('data-src')\n",
    "        pid = td[0].find('img').get('id')\n",
    "        nationality = td[1].find('a').get('title')\n",
    "        flag_img = td[1].find('img').get('data-src')\n",
    "        name = td[1].findAll('a')[1].text\n",
    "        age = td[2].find('div').text.strip()\n",
    "        overall = td[3].text.strip()\n",
    "        potential = td[4].text.strip()\n",
    "        club = td[5].find('a').text\n",
    "        club_logo = td[5].find('img').get('data-src')\n",
    "        value = td[6].text.strip()\n",
    "        wage = td[7].text.strip()\n",
    "        special = td[8].text.strip()\n",
    "        player_data = pd.DataFrame([[pid, name, age, picture, nationality, flag_img, overall, \n",
    "                                  potential, club, club_logo, value, wage, special]])\n",
    "        player_data.columns = columns\n",
    "        data = data.append(player_data, ignore_index=True)\n",
    "    \n",
    "    offset+=1\n",
    "    #data.to_csv('data/full_player_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/basic_player_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25500, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_player_data = pd.read_csv(\"data/basic_player_data.csv\")\n",
    "basic_player_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a dataset with over 25000 players. Now let scrap more detailed data for each player. We will get the rating in each category like for example: shoot, pace, drible, GK reflexes and etc.\n",
    "Also we will scrap how good is a player(overall) in a certain position.\n",
    "\n",
    "We will save the data in a dataset called \"player_ratings_data.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_url = 'https://sofifa.com/player/'\n",
    "master_data = pd.DataFrame()\n",
    "r = 0\n",
    "for index, row in data.iterrows():\n",
    "    skill_names = []\n",
    "    skill_map = {'ID' : str(row['ID'])}\n",
    "    url = player_data_url + str(row['ID'])\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text,\"html.parser\")\n",
    "    categories = soup.findAll('div', {'class': 'column col-4 mb-20'})\n",
    "    for category in categories[:-1]:\n",
    "        skills = category.findAll('li')\n",
    "        for skill in skills:\n",
    "            name_value_pair = skill.text.split()\n",
    "            name_value_pair.reverse()\n",
    "            value = name_value_pair.pop()\n",
    "            name_value_pair.reverse()\n",
    "            n = ' '.join(name_value_pair)\n",
    "            skill_names.append(n)\n",
    "            skill_map[str(n)] = value\n",
    "    attr_data = pd.DataFrame(columns=skill_names)\n",
    "    for key in skill_map.keys():\n",
    "        attr_data.loc[r,key] = skill_map[key]\n",
    "    r = r + 1\n",
    "    master_data = pd.concat([master_data, attr_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_data.to_csv('data/player_ratings_data.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
